[

  {
    "title": "Analysis of algorithms",
    "content": "<p>Is the determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them. Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Big O notation",
    "content": "<p><b>Big O notation</b> is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity in the worst-case scenario. The order of the function is usually the value of the Big O.</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "NP complexity",
    "content": "<p><b>NP (nondeterministic polynomial time)</b> is a complexity class used to classify decision problems. NP is the set of decision problems for which the problem instances, where the answer is \"yes\", have proofs verifiable in polynomial time<br><br>NP is the set of decision problems solvable in polynomial time by a non-deterministic Turing machine. </p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Stooge Sort",
    "content": "<p><b>Stooge sort</b> is a recursive sorting algorithm. It is notable for its exceptional bad time complexity of O(n log 3 / log 1.5 ) = O(n ^ 2.7095...). The algorithm is defined as follows</p>",
    "image_url": "images/stooge.png",
    "deck": "Algorithms"
  },
  {
    "title": "Algorithms classifications",
    "content": "<p>> Computational complexity (worst, average and best behavior)<br>> Computational complexity of swaps (for \"in-place\" algorithms)<br>>Memory usage<br>> Recursion<br>> Stability<br>> General method: insertion, exchange, selection, merging, etc.<br>> Whether the algorithm is serial or parallel.<br>> Adaptability: Whether or not the presortedness of the input affects the running time</p>",
    "image_url": "",
    "deck": "Algorithms"
  },

  {
    "title": "Pigeonhole Sort",
    "content": "<p><b>Pigeonhole Sort</b> is no based on comparing. It works as follows<br><br>> Given an array of values to be sorted, set up an auxiliary array of initially empty \"pigeonholes\", one pigeonhole for each key through the range of the original array.<br>> Going over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.<br>> Iterate over the pigeonhole array in order, and put elements from non-empty pigeonholes back into the original array.</p><p>Worst-case scenario O(n) (The range must be known)",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Cycle Sort",
    "content": "<p><b>Cycle sort</b> is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm.<br><br>> If the element is already at the correct position, do nothing.<br>> If it is not, we will write it to its intended position. That position is inhabited by a different element b, which we then have to move to its correct position. This process of displacing elements to their correct positions continues until an element is moved to the original position of a. This completes a cycle.</p><p>Worst-case scenario O(n * n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Cocktail Sort",
    "content": "<p><b>Cocktail shaker sort</b>, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort. The algorithm differs from a bubble sort in that it sorts in both directions on each pass through the list.</p><p>Worst case scenario O(n * n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Strand Sort",
    "content": "<p><b>Strand Sort</b> is an algorithm that uses a new array to compare and merge values if needed.<br><br>>Create an empty sublist and move first item of input_array[] to it.<br>>Traverse remaining items of input_array. For every item x, check if x is bigger than last inserted item to sublist. If it is, remove x from input_array and add at the end of sublist. If not, keep it in input_array<br>> Merge sublist into output_list<br>> Recur for remaining items in input_array and current items in output_array.</p><p>Worst case scenario O(n * n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Tree Sort",
    "content": "<p>A <b>tree sort</b> is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order. Its typical use is sorting elements online: after each insertion, the set of elements seen so far is available in sorted order.</p><p>Worst case scenario O(n * n) - if the tree is unbalanced; O(n log n) - if the tree is balanced</p>",
    "image_url": "",
    "deck": "Algorithms"
  },

  {
    "title": "Quick Sort",
    "content": "<p><b>Quicksort</b> (sometimes called partition-exchange sort) is an efficient sorting algorithm. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort. Worst critics to this algorithm is the selection of the pivot since it is not defined<br><br> >Pick an element, called a pivot, from the array.<br> >Partitioning: reorder the array so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the partition operation.<br> > Recursively apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values.<br><br> Worst case scenario O(n * n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Heap Sort",
    "content": "<p><b>Heapsort</b> is a comparison-based sorting algorithm<br>> In the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions<br>> In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.<br><br> Worst case scenario O(n log n)</p></p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Introsort",
    "content": "<p><b>Introsort</b> is a variant of quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case running time.</p><p><font color=\"blue\">let maxComplex = floor(log(size of array)))<br>if(maxComplex == 0)<br>&nbsp;&nbsp;heapsort(array)<br>else<br>&nbsp;&nbsp;quicksort(array)</font><br><br> Worst case scenario O(n log n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Counting Sort",
    "content": "<p><b>Counting sort</b> is a sorting technique based on keys between a specific range. It works by counting the number of objects having distinct key values (kind of hashing). Then doing some arithmetic to calculate the position of each object in the output sequence. <b>It is not a comparison-based sorting algorithm</b>. It requires to know previously the range of the unsorted numbers<br><br> Worst case scenario O(n + k ) where k is the range of input</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Radix Sort",
    "content": "<p>In computer science, <b>radix sort</b> is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. A positional notation is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, radix sort is not limited to integers. Least significant digit radix sorts typically use the following sorting order: short keys come before longer keys, and then keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.<br><br> Worst case scenario O(w * n) where w is the number of bits required to store each key.</p>",
    "image_url": "",
    "deck": "Algorithms"
  },

  {
    "title": "Sleep Sort",
    "content": "<p>AKA King of Laziness or Sorting While Sleeping. Sleep sort works by starting a separate task (thread) for each item to be sorted, where each task sleeps for an interval corresponding to the item's sort key, then emits the item. Items are then collected sequentially in time by an observer.<br><br>There is not worst case scenario since all scenarios takes at least as much time as the sum of the numbers to sort, although multiple threads running at the same time might reduce the time</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Bucket Sort",
    "content": "<p>Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm.<br><br> Worst case scenario O(n * n)</p>",
    "image_url": "images/bucket_sort.png",
    "deck": "Algorithms"
  },
  {
    "title": "Shell Sort",
    "content": "<p>ShellSort is mainly a variation of Insertion Sort. In insertion sort, we move elements only one position ahead. When an element has to be moved far ahead, many movements are involved. The idea of shellSort is to allow exchange of far items. In shellSort, we make the array h-sorted for a large value of h. We keep reducing the value of h until it becomes 1. An array is said to be h-sorted if all sub-lists of every h’th element is sorted.</p><p>O(n2) (worst known worst case gap sequence); O(n log^2 n) (best known worst case gap sequence)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "TimSort",
    "content": "<p>Timsort was designed to take advantage of runs of consecutive ordered elements that already exist in most real-world data, natural runs. It iterates over the data collecting elements into runs, and simultaneously merging those runs together. When there are runs, doing this decreases the total number of comparisons needed to fully sort the list.</p><p>Timsort iterates over the data looking for natural runs of at least two elements that are either non-descending (each element is greater than or equal to its predecessor) or strictly descending (each element is less than its predecessor). Since descending runs are later blindly reversed, excluding equal elements maintains the algorithm's stability; i.e., equal elements won't be reversed. Note that any two elements are guaranteed to be either descending or non-descending.</p><p>Worst-case performance O(n log n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Comb Sort",
    "content": "<p>Comb Sort is mainly an improvement over Bubble Sort. Bubble sort always compares adjacent values. So all inversions are removed one by one. Comb Sort improves on Bubble Sort by using gap of size more than 1. The gap starts with a large value and shrinks by a factor of 1.3 (chosen by statistics) in every iteration until it reaches the value 1. Thus Comb Sort removes more than one inversion counts with one swap and performs better than Bubble Sort</p><p>Worst-case performance O(n^2) but avg O(n^2 / 2^p) where p is the number of increments</p>",
    "image_url": "",
    "deck": "Algorithms"
  },

  {
    "title": "BogoSort or Permutation Sort",
    "content": "<p>BogoSort also known as permutation sort, stupid sort, slow sort, shotgun sort or monkey sort is a particularly ineffective algorithm based on generate and test paradigm. The function successively generates permutations of its input until it finds one that is sorted.<br><font color=\"blue\">while not Sorted(list) do<br>&nbsp;&nbsp;shuffle (list)<br>done</font></p><p>>Worst Case : O(∞) (since this algorithm has no upper bound)<br>>Average Case: O(n*n!)<br>>Best Case : O(n) (when array given is already sorted)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Insertion Sort",
    "content": "<p>At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.<br>> Works case scenario (n^2) comparisons and swaps</p>",
    "image_url": "images/insertion.png",
    "deck": "Algorithms"
  },
  {
    "title": "Recursive Insertion Sort",
    "content": "<p>Following the same steps for insertion sort, it starts with the first element of the array and tries to insert it in the recursive call response of the rest of the array (with size n-1) until the last call which atomically checks and swaps between two elements.<br>> Works case scenario (n^2) comparisons and swaps</p>",
    "image_url": "images/insertion_r.png",
    "deck": "Algorithms"
  },
  {
    "title": "Merge Sort",
    "content": "<p>In computer science, <b>merge sort</b> (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Merge sort is a divide and conquer algorithm<br><br>> Divide the unsorted list into n sub-lists, each containing one element (a list of one element is considered sorted)<br>> Repeatedly merge sub-lists to produce new sorted sub-lists until there is only one sublist remaining. This will be the sorted list.</p><p>Worst-case performance O(n log n); Space O(n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Iterative Merge Sort",
    "content": "<p>As traditional Merge Sort recursive implementations, it divides the unsorted lists into two sub-lists until it is easier to sort and merges the sorted sub-lists. The iterative version does not require explicit auxiliary stack</p><p>Worst-case performance O(n log n); Space O(n)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },

  {
    "title": "Selection Sort",
    "content": "<p>The selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning.</p><p>Time Complexity: O(n^2) as there are two nested loops.</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "3-Way MergeSort",
    "content": "<p>Merge sort involves recursively splitting the array into 2 parts, sorting and finally merging them. A variant of merge sort is called 3-way merge sort where instead of splitting the array into 2 parts we split it into 3 parts.Merge sort recursively breaks down the arrays to sub-arrays of size half. Similarly, 3-way Merge sort breaks down the arrays to sub-arrays of size one third.</p><li>Time complexity: O(n log3 n)</li> ",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Bubble Sort",
    "content": "<p>It works by multiple iterations repeatedly swapping the adjacent elements if they are in wrong order. In each iteration, part of the array is sorted at the left. This algorithm also runs an extra time after the array is sorted to check if it is done</p><p>Time Complexity at best case scenario: O(n)</p><p>Time Complexity at worst case scenario: O(n^2)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Recursive Bubble Sort",
    "content": "<p>Same as bubble sort, it repeatedly swaps elements setting the biggest number at the right and calling recursively the rest of the array with the same function. The escape clause is an array of size 1. In each iteration it goes from size n, n-1, n-2, ... 1</p><p>Time Complexity at best case scenario: O(n)</p><p>Time Complexity at worst case scenario: O(n^2)</p>",
    "image_url": "",
    "deck": "Algorithms"
  },
  {
    "title": "Pancake Sorting",
    "content": "<p>Based on Selection Sort but with only one function available<br><font color=\"blue\">flip(array, i) //Reverse array from position 0 to i</font></p><p>In each iteration find the smallest number then use flip</p><p>Total O(n) flip operations are performed in above code. The overall time complexity is O(n^2).</p>",
    "image_url": "",
    "deck": "Algorithms"
  }
]
